{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvTranspose2D_examples_solucion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camulro/Aprendizaje-II/blob/sesi%C3%B3n2/ConvTranspose2D_examples_solucion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WndRPSUqP9nN"
      },
      "source": [
        "# ConvTranspose2D examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LdsrymqP7vI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RZq_ZF_Rs2K"
      },
      "source": [
        "# Syntax:\n",
        "# torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, \n",
        "#groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n",
        "\n",
        "# Input: (N,Cin,Hin,Win)\n",
        "# Output: (N,Cout, Hout, Wout)\n",
        "\n",
        "# Donde:\n",
        "# Hout=(Hin−1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\n",
        "# Wout=(Win−1)×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1\n",
        "# N es el tamaño del batch\n",
        "# dilation por defecto es 1\n",
        "\n",
        "# Convertir de 16 canales a 33 usando un kernel cuadrado 3x3, stride=2 y sin padding\n",
        "m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n",
        "# La misma conversión pero con kernel rectangular: H=3, W=5, stride asimétrico (2 en H y 1 en W) y padding asimétrico (4 en H y 2 en W)\n",
        "m2 = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyfnVfbSQMXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88bf3d47-63e2-42c3-e825-40081eceffe1"
      },
      "source": [
        "# Definir un tensor de entrada random\n",
        "input = torch.randn(20, 16, 50, 100)\n",
        "# Visualizar con print las dimensiones de salida de ambas convoluciones transpuestas\n",
        "output = m(input)\n",
        "print(output.shape)\n",
        "# Hout = (50-1)x2 - 2x0 + 1x(3-1) + 0 + 1 = 101\n",
        "# Wout = (100-1)x2 - 2x0 + 1x(3-1) + 0 + 1 = 201 duplicamos anchura y altura de los tamaños originales\n",
        "output2 = m2(input)\n",
        "print(output2.shape)\n",
        "# Hout = (50-1)x2 - 2x4 + 1x(3-1) + 0 + 1 = 93\n",
        "# Wout = (100-1)x1 - 2x2 + 1x(5-1) + 0 + 1 = 100"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 33, 101, 201])\n",
            "torch.Size([20, 33, 93, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_2-44ImQHlM"
      },
      "source": [
        "# Vamos a especificar ahora las dimensiones de salida mediante output_size\n",
        "# Primero define un tensor random de 16 canales e imágenes de  H=12, W=12\n",
        "input = torch.randn(1, 16, 12, 12)\n",
        "# define dos capas Conv2d/ConvTranspose2d que mantengan el nº de canales en 16, kernel 3x3, stride=2 y padding=1\n",
        "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
        "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBqMRmMPS-9I",
        "outputId": "74211cd2-0181-4019-f3c5-4d1d4ec2b664"
      },
      "source": [
        "# Submuestrea input reduciendo sus dimensiones H y W mediante la capa downsample.\n",
        "h = downsample(input)\n",
        "# Observa con print las dimensiones del tensor obtenido.\n",
        "h.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 6, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez5ka_pfQlVF",
        "outputId": "fbc539e2-e7cc-428e-8478-5d8195892ffd"
      },
      "source": [
        "# Salida del upsample del tensor obtenido\n",
        "output = upsample(h)\n",
        "print(output.size())\n",
        "# Podremos ajustar las dimensiones de las features de salida con output_size. \n",
        "# Fuerza a que tengamos el mismo tamaño de features de salida que de entrada.\n",
        "output = upsample(h, output_size=input.size())\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16, 11, 11])\n",
            "torch.Size([1, 16, 12, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Existe ambigüedad en la convolución, por eso podemos especificar el output_size que queramos"
      ],
      "metadata": {
        "id": "NF4fAwYpfotd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMr1UeF9aujY"
      },
      "source": [
        "This compensates equal deconv output sizes for different input feature sizes (differing in 1 unit in H/W). Check that if we do not compensate the output size we obtain the same upsampled size with an input of size 11x11 and with 12x12."
      ]
    }
  ]
}