{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Pytorch-Regresion_Logistica.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ghPSdPkCrLyk",
        "n2QOf-HEltUP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "64b0c6e489124c05bdab8c76818ee358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eff4df3e88364b3a8eb3e100ceab565d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_68e68f1cfb0b4652863b81358d39f053",
              "IPY_MODEL_623aabc9c911493fa7f79c2c34ff5334",
              "IPY_MODEL_276302f27eda48198101d651069a58a5"
            ]
          }
        },
        "eff4df3e88364b3a8eb3e100ceab565d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68e68f1cfb0b4652863b81358d39f053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95108032771b4b9bbfdca46862cd6f06",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5185c15f50ed4c4abc5a7febe602ba92"
          }
        },
        "623aabc9c911493fa7f79c2c34ff5334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50ab82d6079e4516a9eb3330b64efc40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b955047ecdb481c9e8f5236c301409e"
          }
        },
        "276302f27eda48198101d651069a58a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bc317b4f5a864f81a1b649eedb3906ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [00:00&lt;00:00, 16843847.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0343fcc1400242bb9e4476642c9df13a"
          }
        },
        "95108032771b4b9bbfdca46862cd6f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5185c15f50ed4c4abc5a7febe602ba92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50ab82d6079e4516a9eb3330b64efc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b955047ecdb481c9e8f5236c301409e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc317b4f5a864f81a1b649eedb3906ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0343fcc1400242bb9e4476642c9df13a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "281965a44980442b825e5cb0c071432c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_123c954a2f464f1aba27fad0fc79bbfa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2f50226b29e4130b5c0dcbb026c33f5",
              "IPY_MODEL_a8072e77dd354b9aa53752cfb972c70c",
              "IPY_MODEL_5a71319ea1a94c5fa0087bc6420a194a"
            ]
          }
        },
        "123c954a2f464f1aba27fad0fc79bbfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2f50226b29e4130b5c0dcbb026c33f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fa7f6a16829d4185b74d6aceeb4a6a3f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a68e016274c843c8bca278a4cb965e48"
          }
        },
        "a8072e77dd354b9aa53752cfb972c70c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8e80d95fb6564e85ac4cf02637aa5bc4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0e66e6d40fa47e4bac565719f67ad60"
          }
        },
        "5a71319ea1a94c5fa0087bc6420a194a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9036e36f9885499ea8b06105f5de7b5b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:00&lt;00:00, 9960.41it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_417a09312b694792b52e97b7f22194ac"
          }
        },
        "fa7f6a16829d4185b74d6aceeb4a6a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a68e016274c843c8bca278a4cb965e48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e80d95fb6564e85ac4cf02637aa5bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0e66e6d40fa47e4bac565719f67ad60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9036e36f9885499ea8b06105f5de7b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "417a09312b694792b52e97b7f22194ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0abad59d770d45ed86e34b63b2f95b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_782c06d3fecc4b4d84aaabf2e8269c38",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_daa7c109f9ca4ec983d308f958feacb6",
              "IPY_MODEL_1e93ae7147d74b8a84295c90b83b2a8f",
              "IPY_MODEL_12b1f63750434521854e6f7a07f2b968"
            ]
          }
        },
        "782c06d3fecc4b4d84aaabf2e8269c38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "daa7c109f9ca4ec983d308f958feacb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_889803f648c94020bd5909220e6e5247",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_baa121a1426a48e28cbb61b24943e10b"
          }
        },
        "1e93ae7147d74b8a84295c90b83b2a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d4dae1c658d6486dbe1685cff0298a1e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_135400a69bff4917b1d385d030b79729"
          }
        },
        "12b1f63750434521854e6f7a07f2b968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_646154740065423ca71328c60290f81c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:00&lt;00:00, 2957156.87it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0e4b533988e4c42a0e21f111aee90ef"
          }
        },
        "889803f648c94020bd5909220e6e5247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "baa121a1426a48e28cbb61b24943e10b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4dae1c658d6486dbe1685cff0298a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "135400a69bff4917b1d385d030b79729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "646154740065423ca71328c60290f81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0e4b533988e4c42a0e21f111aee90ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a50b79e5d78476ebc255f35003dce34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_42a8293a6cf54840a28ef77c1258d513",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f0d3acead91040a8b0eedbaa0ce9eb50",
              "IPY_MODEL_8ed6429618dd434abd564cf657504ba2",
              "IPY_MODEL_7837273b41c540389c8b19513e519955"
            ]
          }
        },
        "42a8293a6cf54840a28ef77c1258d513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0d3acead91040a8b0eedbaa0ce9eb50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7677d1ad62c4c5f9cd86ca33db3de67",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd03be5456d14932aa357062892cc81f"
          }
        },
        "8ed6429618dd434abd564cf657504ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2ca52895da394141a6ff7e5caff35f19",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb05439eb3ba43528060bfb906b3eec7"
          }
        },
        "7837273b41c540389c8b19513e519955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_078ea9c2dc9340d7905c1344f9082346",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:00&lt;00:00, 9785.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d40d8cf79f144639bb6f8fbf5f8d33b2"
          }
        },
        "d7677d1ad62c4c5f9cd86ca33db3de67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd03be5456d14932aa357062892cc81f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ca52895da394141a6ff7e5caff35f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb05439eb3ba43528060bfb906b3eec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "078ea9c2dc9340d7905c1344f9082346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d40d8cf79f144639bb6f8fbf5f8d33b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camulro/Aprendizaje-II/blob/sesi%C3%B3n1/03_Pytorch_Regresion_Logistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghPSdPkCrLyk"
      },
      "source": [
        "![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \n",
        "\n",
        "#<strong>**Máster en Inteligencia Artificial Avanzada y Aplicada  IA^3**</strong>\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxh4k85SpESm"
      },
      "source": [
        "# <center>**Regresión logística con Pytorch**</center>\n",
        "(clasificar la pertenencia a un grupo de clases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL4niY96fTTV"
      },
      "source": [
        "En este notebook vamos a estudiar los siguientes aspectos: \n",
        "\n",
        "* Trabajar con imagenes en Pytorch (empleando el dataset MNIST)\n",
        "* Dividir el conjunto de datos en entrenamiento, test y validación.\n",
        "* Crear modelos en Pytorch a partir de la clase base `nn.Module` y personalizandola. \n",
        "* Interpretar las salidas del modelo coomo probabilidades empleando la fnción softmax y asignar etiquetas en predicción\n",
        "* Emplear métricas de evaluación y funciones de error adecuadas para problemas de clasificación\n",
        "* Desarrollar el proceso de entrernamiento que también evalue el modelo con cel conjunto de validación\n",
        "* Probar el modelo manualmente con ejemplos \n",
        "* Salvar y cargar el modelo para evitar reentrenar desde cero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlizBg8UfTTV"
      },
      "source": [
        "## Trabajando con imagenes\n",
        "\n",
        "En este ejemplo vamos a aprovechar lo que ya sabemos sobre Pytorch y la regresion lineal para aplicarlo a un tipo de problema diferente: *clasificación* en este caso de imágenes. Para ello vamos a usar la conocida [*MNIST Handwritten Digits Database*](http://yann.lecun.com/exdb/mnist/) como nuestro conjunto de datos. Consiste en imágenes de 28px por 28px en escala de grises de dígitos escritos a mano (0 a 9) y sus respectivas etiquetas indicando que dígito representa cada imagen. \n",
        "\n",
        "El dataset tiene este aspecto:\n",
        "\n",
        "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IArll2smfTTV"
      },
      "source": [
        "Empezaremos por importar `torch` y `torchvision`. `torchvision` contiene utilidades para trabajar con imágenes y clases muy útiles para descargar e importar automáticamente conjuntos de datos habituales como MNIST.\n",
        "\n",
        "(Puedes consultar la [documentación](https://pytorch.org/vision/stable/datasets.html) para ver todos los datasets disponibles a traves de esta librería)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_D8CGr0fTTV"
      },
      "source": [
        "# Si ejecutas el notebook de forma local, descomenta la linea que corresponda a tu sistema operativo para instalar las libreriás correspondientes\n",
        "\n",
        "# Linux / Binder\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Windows\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS\n",
        "# !pip install numpy matplotlib torch torchvision torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_xViEWvfTTV"
      },
      "source": [
        "# Importamos\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Skz7dlfTTV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422,
          "referenced_widgets": [
            "64b0c6e489124c05bdab8c76818ee358",
            "eff4df3e88364b3a8eb3e100ceab565d",
            "68e68f1cfb0b4652863b81358d39f053",
            "623aabc9c911493fa7f79c2c34ff5334",
            "276302f27eda48198101d651069a58a5",
            "95108032771b4b9bbfdca46862cd6f06",
            "5185c15f50ed4c4abc5a7febe602ba92",
            "50ab82d6079e4516a9eb3330b64efc40",
            "3b955047ecdb481c9e8f5236c301409e",
            "bc317b4f5a864f81a1b649eedb3906ab",
            "0343fcc1400242bb9e4476642c9df13a",
            "281965a44980442b825e5cb0c071432c",
            "123c954a2f464f1aba27fad0fc79bbfa",
            "d2f50226b29e4130b5c0dcbb026c33f5",
            "a8072e77dd354b9aa53752cfb972c70c",
            "5a71319ea1a94c5fa0087bc6420a194a",
            "fa7f6a16829d4185b74d6aceeb4a6a3f",
            "a68e016274c843c8bca278a4cb965e48",
            "8e80d95fb6564e85ac4cf02637aa5bc4",
            "a0e66e6d40fa47e4bac565719f67ad60",
            "9036e36f9885499ea8b06105f5de7b5b",
            "417a09312b694792b52e97b7f22194ac",
            "0abad59d770d45ed86e34b63b2f95b0c",
            "782c06d3fecc4b4d84aaabf2e8269c38",
            "daa7c109f9ca4ec983d308f958feacb6",
            "1e93ae7147d74b8a84295c90b83b2a8f",
            "12b1f63750434521854e6f7a07f2b968",
            "889803f648c94020bd5909220e6e5247",
            "baa121a1426a48e28cbb61b24943e10b",
            "d4dae1c658d6486dbe1685cff0298a1e",
            "135400a69bff4917b1d385d030b79729",
            "646154740065423ca71328c60290f81c",
            "a0e4b533988e4c42a0e21f111aee90ef",
            "9a50b79e5d78476ebc255f35003dce34",
            "42a8293a6cf54840a28ef77c1258d513",
            "f0d3acead91040a8b0eedbaa0ce9eb50",
            "8ed6429618dd434abd564cf657504ba2",
            "7837273b41c540389c8b19513e519955",
            "d7677d1ad62c4c5f9cd86ca33db3de67",
            "fd03be5456d14932aa357062892cc81f",
            "2ca52895da394141a6ff7e5caff35f19",
            "fb05439eb3ba43528060bfb906b3eec7",
            "078ea9c2dc9340d7905c1344f9082346",
            "d40d8cf79f144639bb6f8fbf5f8d33b2"
          ]
        },
        "outputId": "8f8a467a-8bee-479b-8476-43d3e695a6f9"
      },
      "source": [
        "# Descargamos el training dataset\n",
        "dataset = MNIST(root='data/', download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64b0c6e489124c05bdab8c76818ee358",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "281965a44980442b825e5cb0c071432c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0abad59d770d45ed86e34b63b2f95b0c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a50b79e5d78476ebc255f35003dce34",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koi-fVbAfTTV"
      },
      "source": [
        "Una vez ejecutado el código anterior, los datos han sido descargados al directorio `data/` vinculado al notebook  y se ha creado un dataset PyTorch `Dataset`. En siguientes ejecuciones, la descarga será omitida ya que los datos ya están descargados. \n",
        "\n",
        "Comprobamos el tamaño del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9mWyKqAfTTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5894acd-9703-4687-c315-f00de54e4775"
      },
      "source": [
        "len(dataset) #tenemos 60000 imágenes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pRKB16NfTTW"
      },
      "source": [
        "El dataset tiene 60,000 imagenes que vamos a emplear para entrenar el modelo. Disponemos también de un conjunto adicional de test de 10,000 imagenes que será usado para evaluar el modelo y establecer métricas en artículos y/o informes. \n",
        "\n",
        "Podemos crear el conjunto de test empleando la clase `MNIST` pasando el argumento `train=False` al constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLqZQOlWfTTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5bbd44-bdb7-45e1-b05d-051e4c69c025"
      },
      "source": [
        "test_dataset = MNIST(root='data/', train=False)\n",
        "len(test_dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgJru-f7fTTW"
      },
      "source": [
        "Observemos un ejemplo del training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZmSZeFGfTTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cba124a-33ae-4262-a22a-fc26da597bfe"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=L size=28x28 at 0x7F06CD4C63D0>, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oBdGAc4fTTX"
      },
      "source": [
        "Vemos que el ejemplo consiste en un par de elementos. Una imagen de 28x28 y una etiqueta indicando el dígito que representa. La imagen es un objeto de la clase `PIL.Image.Image`, la cual es parte de la *Python imaging library* [Pillow](https://pillow.readthedocs.io/en/stable/). Podemos visualizar la imagen en Jupyter usando [`matplotlib`](https://matplotlib.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edyBIgXPfTTX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "#queremos ver los gráficos en el propio notebook"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSVQL5vJfTTX"
      },
      "source": [
        "La instrucción `%matplotlib inline` indica a Jupyter que queremos que muestre el gráfico en el propio notebook. Sin esa línea, la imagen se mostraría en una ventana popup. Las instrucciones que se inician con `%` son las denominadas *magic commands* y se emplean para configurar el comportamiento del propio Jupyter. Y Puedes encontrar el listado completo de estos comandos aquí: https://ipython.readthedocs.io/en/stable/interactive/magics.html .\n",
        "\n",
        "Veamos un par de ejemplos del dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cxwpTQLfTTX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ac1708bc-bb1d-4a96-c4a4-250297a7aaed"
      },
      "source": [
        "image, label = dataset[112]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label) #28x28 pixeles"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ELPbM7fTTX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "edb6fae5-7e05-4de7-8239-00f4a7762f4f"
      },
      "source": [
        "image, label = dataset[1499]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANdElEQVR4nO3dXahd9ZnH8d9vYnJjaoiKh5iorTUqQfB0OMhIg2SoLZmgJHohDRIzUeZ4UcVILyY4aCVXIqbBFyimGptKx9pQNSK1oyNCyE1JDJm8SWumJjThmEwJoVHBmJxnLs5KOerZ/33ce+2Xk+f7gcPeez177fWwyS9r7fX2d0QIwLnvH3rdAIDuIOxAEoQdSIKwA0kQdiCJ87q5MNvs+gc6LCI80fS21uy2F9v+o+0Dtte081kAOsutHme3PU3SnyR9X9JhSdslLY+I/YV5WLMDHdaJNfsNkg5ExJ8j4pSkX0ta2sbnAeigdsI+V9Jfxr0+XE37AtvDtnfY3tHGsgC0qeM76CJig6QNEpvxQC+1s2Y/Iumyca/nVdMA9KF2wr5d0nzb37I9Q9IPJb1eT1sA6tbyZnxEnLZ9n6T/kjRN0saI2FdbZwBq1fKht5YWxm92oOM6clINgKmDsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEy+OzS5Ltg5JOSjoj6XREDNXRFID6tRX2yj9HxF9r+BwAHcRmPJBEu2EPSW/Zfs/28ERvsD1se4ftHW0uC0AbHBGtz2zPjYgjti+R9Lak+yNia+H9rS8MwKREhCea3taaPSKOVI/HJL0q6YZ2Pg9A57Qcdtvn2/7G2eeSfiBpb12NAahXO3vjByS9avvs5/xnRPy+lq4A1K6t3+xfe2H8Zgc6riO/2QFMHYQdSIKwA0kQdiAJwg4kUceFMEhsYGCgWF+5cmXD2q233lqcd+HChcX6ww8/XKzv2bOnYW3Lli3Fec9FrNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAmuepsCZsyYUazfeeedXerkq6644opi/ZFHHulSJ1916tSphrUHHnigOO+zzz5bdztdw1VvQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AEx9n7wNBQefDb+++/v1hfsWJFne2ksHVrw4GLJEmLFi3qTiMdwHF2IDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC+8Z3weLFi4v1F198sVi/6KKL6mznaynde12SPv3005Y/e9++fcX63Xff3fJnS9Lx48cb1p544om2Pnsqarpmt73R9jHbe8dNu9D227Y/qB5nd7ZNAO2azGb8LyR9edW0RtI7ETFf0jvVawB9rGnYI2KrpC9vDy2VtKl6vknSspr7AlCzVn+zD0TESPX8I0kNB/yyPSxpuMXlAKhJ2zvoIiJKF7hExAZJGyQuhAF6qdVDb0dtz5Gk6vFYfS0B6IRWw/66pLNj8a6UlG/8W2CKaboZb/slSYskXWz7sKSfSHpM0m9s3yPpkKQ7OtnkVHfvvfcW682Oo4+OjhbrBw8ebFibNWtWcd5t27YV66tWrSrWT5w4Uayfd17jf2LPPfdccd52vfvuuw1rb7zxRkeX3Y+ahj0iljcofa/mXgB0EKfLAkkQdiAJwg4kQdiBJAg7kAS3ku6CgYGGZxNLkpYtK19aUBp6WJJeeOGFhrXrrruuOO+HH35YrH/yySfF+rRp04r1+fPnN6zt37+/OG8zJ0+eLNaHhxufpf3yyy+3tex+xq2kgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJjrOjLYODg8X6zp07O7bsZpfvNjsOf67iODuQHGEHkiDsQBKEHUiCsANJEHYgCcIOJMGQzSjavn17sX799de3/Nlnzpwp1m+77bZi/eOPP2552RmxZgeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJLieHUWXXHJJsX755ZcX6w8++GDD2muvvVacd/PmzcU6Jtby9ey2N9o+ZnvvuGmP2j5ie1f1t6TOZgHUbzKb8b+QtHiC6esjYrD6+129bQGoW9OwR8RWSce70AuADmpnB919tndXm/mzG73J9rDtHbZ3tLEsAG1qNew/k/RtSYOSRiSta/TGiNgQEUMRMdTisgDUoKWwR8TRiDgTEaOSfi7phnrbAlC3lsJue864l7dJ2tvovQD6Q9Pr2W2/JGmRpIttH5b0E0mLbA9KCkkHJd3bwR7RQ1deeWWxfu211xbrCxYsaFg7cOBAcd6hofIvv7Vr1xbrzcaWz6Zp2CNi+QSTn+9ALwA6iNNlgSQIO5AEYQeSIOxAEoQdSIJbSfeBu+66q1i/5pprivWnn366Ya3Z7ZpvueWWYv3JJ58s1mfOnFmsf/755w1rl156aXHet956q1j/7LPPinV8EWt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCW0lPAdOnTy/Wr7766oa1m2++uTjv+vXrW+rprEOHDhXrq1evblg7fPhwcd7SMXpJ2r17d7GeVcu3kgZwbiDsQBKEHUiCsANJEHYgCcIOJEHYgSS4nn0KOH36dLF+1VVXNazdfvvtbS17dHS0WN+4cWOxvmXLlraWj/qwZgeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJLiefQq44IILivUTJ050bNlr1qwp1h9//PGOLRutafl6dtuX2X7X9n7b+2w/UE2/0Pbbtj+oHmfX3TSA+kxmM/60pB9HxAJJ/yTpR7YXSFoj6Z2ImC/pneo1gD7VNOwRMRIRO6vnJyW9L2mupKWSNlVv2yRpWaeaBNC+r3VuvO1vSvqOpD9IGoiIkar0kaSBBvMMSxpuvUUAdZj03njbMyX9VtLqiPjb+FqM7eWbcOdbRGyIiKGIGGqrUwBtmVTYbU/XWNB/FRGvVJOP2p5T1edIOtaZFgHUoelmvG1Lel7S+xHx03Gl1yWtlPRY9ci1jC2aNWtWsb5q1aqOLXvz5s3F+rp16zq2bHTXZH6zf1fSCkl7bO+qpj2ksZD/xvY9kg5JuqMzLQKoQ9OwR8Q2SRMepJf0vXrbAdApnC4LJEHYgSQIO5AEYQeSIOxAElzi2geWLStfVvDKK68U6yUHDhwo1pcsWdLW/Og/DNkMJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0kwZHMfWLFiRcc++5lnninWOY6eB2t2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC4+xdcNNNNxXrN954Y7E+OjparD/11FMNa82OsyMP1uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kETT+8bbvkzSLyUNSApJGyLiSduPSvo3Sf9XvfWhiPhdk886J+8bPzg4WKy/+eabxfrAwECxPjIyUqzPnTu3WEcuje4bP5mTak5L+nFE7LT9DUnv2X67qq2PiCfqahJA50xmfPYRSSPV85O235fEqgSYYr7Wb3bb35T0HUl/qCbdZ3u37Y22ZzeYZ9j2Dts72uoUQFsmHXbbMyX9VtLqiPibpJ9J+rakQY2t+ddNNF9EbIiIoYgYqqFfAC2aVNhtT9dY0H8VEa9IUkQcjYgzETEq6eeSbuhcmwDa1TTsti3peUnvR8RPx02fM+5tt0naW397AOoymb3x35W0QtIe27uqaQ9JWm57UGOH4w5KurcjHU4B8+bNK9abHVprZu3atW3ND0iT2xu/TdJEx+2Kx9QB9BfOoAOSIOxAEoQdSIKwA0kQdiAJwg4k0fQS11oXdo5e4gr0k0aXuLJmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkuj1k818lHRr3+uJqWj/q1976tS+J3lpVZ29XNCp09aSaryzc3tGv96br1976tS+J3lrVrd7YjAeSIOxAEr0O+4YeL7+kX3vr174kemtVV3rr6W92AN3T6zU7gC4h7EASPQm77cW2/2j7gO01veihEdsHbe+xvavX49NVY+gds7133LQLbb9t+4PqccIx9nrU26O2j1Tf3S7bS3rU22W237W93/Y+2w9U03v63RX66sr31vXf7LanSfqTpO9LOixpu6TlEbG/q400YPugpKGI6PkJGLZvkvSxpF9GxHXVtMclHY+Ix6r/KGdHxL/3SW+PSvq418N4V6MVzRk/zLikZZL+VT387gp93aEufG+9WLPfIOlARPw5Ik5J+rWkpT3oo+9FxFZJx780eamkTdXzTRr7x9J1DXrrCxExEhE7q+cnJZ0dZryn312hr67oRdjnSvrLuNeH1V/jvYekt2y/Z3u4181MYCAiRqrnH0lqb2yp+jUdxrubvjTMeN98d60Mf94udtB91cKI+EdJ/yLpR9Xmal+Ksd9g/XTsdFLDeHfLBMOM/10vv7tWhz9vVy/CfkTSZeNez6um9YWIOFI9HpP0qvpvKOqjZ0fQrR6P9bifv+unYbwnGmZcffDd9XL4816Efbuk+ba/ZXuGpB9Ker0HfXyF7fOrHSeyfb6kH6j/hqJ+XdLK6vlKSVt62MsX9Msw3o2GGVePv7ueD38eEV3/k7REY3vk/1fSf/SihwZ9XSnpf6q/fb3uTdJLGtus+1xj+zbukXSRpHckfSDpvyVd2Ee9vShpj6TdGgvWnB71tlBjm+i7Je2q/pb0+rsr9NWV743TZYEk2EEHJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0n8P2ATQGkio/G+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vO2Gh9RfTTX"
      },
      "source": [
        "Observamos que las imagenes que tenemos son relativamente pequeñas y por tanto el reconocimento del dígito puede llegar a ser dificil, incluso para un ojo humano. No obstante, este dataset no deja de ser muy útil. \n",
        "\n",
        "Llegado este puntom el principal problema es que Pytorch no sabe como trabajar con imagenes. De forma similar a otras librerías DL, será necesario transformar las imágenes a tensores. Podemos realizar esto directamente cuando creamos el dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBnad_UDfTTX"
      },
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "084T0emtfTTX"
      },
      "source": [
        "PyTorch datasets nos permite especificar una o más transformaciones que se aplican a las imagenes directamente cuando son cargadas. El módulo `torchvision.transforms` contiene esas funciones predefinidas. Vamos a emplear la transformación `ToTensor` para convertir las imágenes a tensores PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYvBQ3LzfTTX"
      },
      "source": [
        "# MNIST dataset (imagenes and labels)\n",
        "dataset = MNIST(root='data/', \n",
        "                train=True,\n",
        "                transform=transforms.ToTensor()) #carga del dataset como antes pero transformando el array a tensor"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCZ4h7jMfTTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5a5407-6557-40fc-d05c-2effa4eb11c2"
      },
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape, label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28]) 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io317DfSfTTX"
      },
      "source": [
        "Ahora la imagen ya es un tensor de dimensiones 1x28x28. La primera dimensión indica los canales de color. La segunda y la tercera implican la altura y anchura de la imagen, respectivamente. Como las imágenes son en escala de grises, solo hay un canal de color. Otros conjuntos de datos con imágenes en colo tendrían 3 canales: rojo, verde y azul (RGB).\n",
        "\n",
        "Veamos algunos valores del interior del tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prZFgdtefTTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af080018-3452-4c98-c169-a6a9c13ad7c0"
      },
      "source": [
        "print(img_tensor[0,10:15,10:15])\n",
        "print(torch.max(img_tensor), torch.min(img_tensor))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
            "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
            "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
            "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
            "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
            "tensor(1.) tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(img_tensor)  # Todos los valores que componen el tensor"
      ],
      "metadata": {
        "id": "MQHP3Rht7LsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c54969-84c7-40db-c5b7-64f10df816ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
            "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
            "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
            "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
            "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
            "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
            "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
            "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
            "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
            "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
            "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
            "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
            "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
            "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
            "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
            "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
            "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
            "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
            "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2HjQ5P5fTTX"
      },
      "source": [
        "La segunda línea de código nos muestra el rango de valores, que va de 0 a 1. O representa el negro, 1 el blanco,  y los valores intermedios representan los diferentes tonos de grises. \n",
        "\n",
        "Veamos gráficamente el tensor como imagen empleando `plt.imshow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw-NvsqpfTTX"
      },
      "source": [
        "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img_tensor[0,0:-1,0:-1], cmap='gray');"
      ],
      "metadata": {
        "id": "eZcylHwT7YiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSKTZ8aifTTX"
      },
      "source": [
        "## Conjuntos de entrenamiento, test y validación\n",
        "\n",
        "Como ya sabemos, cuando construimos modelos reales de aprendizaje máquina, es habitual dividir el conjunto de datos en tres partes: \n",
        "\n",
        "1. **Training set** - empleado para entrenar el modelo. Es con el que calculamos el error y usamos para ajustar los pesos empleando el algoritmo de optimización elegido (p.e. descenso de gradiente).\n",
        "2. **Validation set** - se emplea para evaluar el modelo durante el entrenamiento, ajustar los hiperparámetros (como learning rate, etc.) y de esa forma escoger la mejor versión obtenida. \n",
        "3. **Test set** - empleado para comparar diferentes modelos entrenados y para aproximar el modelo a su eficacia final con nuevos datos. .\n",
        "\n",
        "En el conjunto MNIST tenemos 60.000 imágenes para entrenamiento y 10.000 para test. El conjunto de test está específicamente preparado, de forma que diferentes investigadores puedan comprobar y reportar sus resultados contra la misma colección de imagenes.  \n",
        "\n",
        "Dado que no hay un conjunto predefinido de validación, lo vamos a obtener nosotros manualmente dividiendo el conjunto de entrenamiento. Vamos a separar 10.000 imágenes aleatoriamente para validación. Para ello emplearemos el método `random_spilt` de PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dcE0OPBfTTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bdebfa-75b5-4c7f-a1ba-5e567aab04ae"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [50000, 10000]) #separa en 50000 y 10000 aleatoriamente\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CACcvMnZfTTX"
      },
      "source": [
        "Es fundamental que la división de los conjuntos sea **aleatoria**. A menudo, el conjunto de entrenamiento puede estar ordenado de alguna forma. Si nos limitaramos a tomar el ultimo 20% de las imágenes, podriamos estar dejando fuera tanto de entrenamiento como de validación algunas etiquetas. Esto nos llevaría a un modelo defectuoso, que estaría mal entrenado y no sería nada útil.\n",
        "\n",
        "Llegado este punto ya podemos crear data loaders que nos van ayudar a cargar los datos en lotes. En este caso emplearemos lotes de 128 elementos. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_oD7BTfTTX"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True) #ahora tenemos 391 items de lotes de 128 elementos\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCtbVhkOfTTX"
      },
      "source": [
        "Recordemos que hemos de poner `shuffle=True` para el dataLoader de entrenamiento de forma que aseguremos que los lotes que se generan en cada ciclo (epoch) son diferentes. Esta alatorización ayuda a entrenar más rápido y a generalizar mejor. Por otro lado, dado que el conjunto de validación únicamente es emmpleado para evaluar el modelo, no es necesario barajar las imágenes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy-lajKQfTTY"
      },
      "source": [
        "## Modelo\n",
        "\n",
        "A continuación vamos a preparar nuestro modelo, teniendo en cuenta las siguientes consideraciones: \n",
        "\n",
        "* Un modelo de  **regresión logística** es practicamente idéntico a un modlo de regresión lineal. Contiene matrices de pesos y biases y su salida se obtiene realizando las mismas operaciones básicas (`pred = x @ w.t() + b`). \n",
        "\n",
        "* De igual forma que con la regresion lineal, podemos emplear la clase base `nn.Linear` para crear el modelo en lugar de crear e inicializar manualmente las matrices.\n",
        "\n",
        "* Dado que `nn.Linear` espera que cada muestra de entrenamiento sea un vector, ahora cada tensor imagen de `1x28x28` tiene que ser \"extendido\" (_flattened_) a un vector de tamaño 784 `(28*28)` antes de ser pasado al modelo. \n",
        "\n",
        "* Finalmente, en este caso la salida para la imagen ya no es un solo valor, sino un vector de tamaño 10, en el que cada elemento significa la probabilidad de que el elemento de entrada sea esa etiqueta en particular (i.e., 0 al 9). La etiqueta predicha será simplemente aquella con el valor de probabilidad más alto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzVbD7pifTTY"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 28*28 # 784\n",
        "num_classes = 10\n",
        "\n",
        "# Modelo de regresión lineal\n",
        "model = nn.Linear(input_size, num_classes)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ilamccMfTTY"
      },
      "source": [
        "Este modelo es mucho más grande que el modelo anterior en términos del número de parametros que hay que ajustar (ahora tenemos 10 salidas en lugar de una). \n",
        "\n",
        "Veamos los pesos y biases: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAj8gVdUfTTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd433486-bced-4981-b8b6-61b166ee0df8"
      },
      "source": [
        "print(model.weight.shape)\n",
        "model.weight"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 784])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0287,  0.0008, -0.0102,  ..., -0.0339,  0.0177, -0.0031],\n",
              "        [ 0.0022, -0.0103, -0.0292,  ...,  0.0005,  0.0131,  0.0138],\n",
              "        [-0.0210,  0.0098, -0.0306,  ...,  0.0168,  0.0112, -0.0203],\n",
              "        ...,\n",
              "        [ 0.0239, -0.0106,  0.0040,  ..., -0.0340,  0.0232,  0.0028],\n",
              "        [-0.0255, -0.0078, -0.0191,  ..., -0.0127,  0.0021, -0.0220],\n",
              "        [ 0.0192, -0.0033, -0.0025,  ...,  0.0121, -0.0322,  0.0050]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRnviQ34fTTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad4ec22-c847-4466-f0be-a14aed4c6ec5"
      },
      "source": [
        "print(model.bias.shape)\n",
        "model.bias"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.0317,  0.0314, -0.0212,  0.0132, -0.0103, -0.0154, -0.0295,  0.0282,\n",
              "        -0.0341, -0.0144], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is6yNefafTTY"
      },
      "source": [
        "Aunque ahora tenemos un total de 7850 (784 valores de entrada x10 salidas+10 del bias) parametros, conceptualmente no hay nada muy diferente. Vamos a generar algunos resultados con el modelo que hemos definido. \n",
        "Vamos a emplear el primer lote de 128 imágenes del dataset y lo pasamos a nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIuxYOy3fTTY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "8f5e1aa4-6063-49ca-d2c9-f6e554ec45f2"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(labels)\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    print(outputs)\n",
        "    break\n",
        "    #da error porque necesita que convirtamos las imágenes a vectores de 784"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9, 5, 4, 6, 9, 5, 3, 4, 1, 3, 1, 2, 3, 6, 8, 7, 0, 0, 6, 0, 7, 7, 2, 3,\n",
            "        6, 8, 4, 3, 4, 9, 1, 1, 4, 9, 3, 9, 5, 0, 7, 7, 4, 4, 0, 6, 7, 0, 9, 9,\n",
            "        9, 2, 7, 6, 2, 1, 6, 9, 0, 1, 9, 4, 3, 2, 5, 2, 2, 4, 7, 5, 2, 9, 7, 8,\n",
            "        1, 1, 3, 2, 0, 3, 4, 9, 8, 0, 3, 8, 3, 7, 4, 8, 7, 3, 9, 5, 7, 1, 1, 6,\n",
            "        2, 1, 9, 6, 0, 6, 6, 0, 9, 5, 3, 9, 0, 8, 2, 8, 0, 7, 4, 7, 8, 0, 7, 6,\n",
            "        0, 5, 9, 5, 5, 4, 3, 3])\n",
            "torch.Size([128, 1, 28, 28])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d0fe7d306f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-JsBr6lSO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1728d96-6732-40d5-a136-70e7ae1486be"
      },
      "source": [
        "images.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3_ktSR1lVJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78df49a9-9e8f-4f6b-cfe0-78050476a598"
      },
      "source": [
        "images.reshape(128, 784).shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lT2ZhMjfTTY"
      },
      "source": [
        "El códio anterior nos ha producido erro porque los datos de entrada no están en la forma correcta. Nuestras imágenes son 1x28x28, pero nosotros las necesitamos como vectores de tamaño 784, i.e. necesitamos \"extenderlas\". Para ello emplearemos el método de tensores `.reshape`, que nos va a permitir eficientemente generar una vista de cada imagen como un vector, sin realmente crear una copia de los datos.  \n",
        "\n",
        "Para incluir esta funcionalidad en nuestro modelo, necesitamos definir un modelo personalizado extendiendo la clase `nn.Module` de PyTorch. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2HWol7qfTTY"
      },
      "source": [
        "class MnistModel(nn.Module): #module es uno de los contenedores básicos que nos permiten meter capas\n",
        "    def __init__(self):\n",
        "        super().__init__() #hereda todos los métodos de la clase anterior\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784) #cambia la forma de las entradas xb a 784 columnas (-1 es un inferidor)\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "    \n",
        "model = MnistModel()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZKn3GBFfTTY"
      },
      "source": [
        "En el interior del constructor `__init__`, lo que hacemos ens instanciar los pesos y biases empleando `nn.Linear`, de igual forma que lo que hemos visto anteriormente. A continuación preparamos un nuevo método `forward`, el cual toma un lote de entradas cuando es invocado, lo \"extiende\" al formato de tensor de entrada correcto, y lo pasa a la función `self.linear`.\n",
        "\n",
        "`xb.reshape(-1, 28*28)` indica a PyTorch que quremos una *vista* del tensor `xb` en dos dimensiones. La longitud de la segunda dimensión la hemos especificado nosotros (28x28=784). Un argumento de `.reshape` puede ser puesto a `-1` (en nuestro caso, la primera dimensión) para hacer que PyTorch calcule automáticamente el valor necesario, basado en la forma especificada.\n",
        "\n",
        "Nótese que el modelo ya no tiene los atributos `.weight` y `.bias` (ahora están dentro del attributo `.linear`), pero sí que tiene un método `.parameters` que retorna na lista conteniendo los pesos y biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voZF2M_3nVnI"
      },
      "source": [
        "model.linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUlyKm3pPMmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44601378-6fdf-41a2-aaf7-45bc2823099e"
      },
      "source": [
        "model.linear.bias\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.0068,  0.0013, -0.0276, -0.0322, -0.0212, -0.0210, -0.0306, -0.0133,\n",
              "         0.0229,  0.0066], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JciejPo9fTTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b1a927-c66d-4670-aa7b-47a9e9ae494f"
      },
      "source": [
        "print(model.linear.weight.shape, model.linear.bias.shape)\n",
        "list(model.parameters())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 784]) torch.Size([10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0021,  0.0119,  0.0141,  ...,  0.0318,  0.0130, -0.0024],\n",
              "         [-0.0145, -0.0057,  0.0106,  ...,  0.0146, -0.0109,  0.0356],\n",
              "         [-0.0237, -0.0256, -0.0195,  ..., -0.0012, -0.0262,  0.0129],\n",
              "         ...,\n",
              "         [-0.0087, -0.0156, -0.0269,  ..., -0.0129, -0.0272,  0.0183],\n",
              "         [ 0.0245, -0.0158, -0.0089,  ...,  0.0011, -0.0295, -0.0132],\n",
              "         [ 0.0278,  0.0123, -0.0220,  ...,  0.0083, -0.0008, -0.0087]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0068,  0.0013, -0.0276, -0.0322, -0.0212, -0.0210, -0.0306, -0.0133,\n",
              "          0.0229,  0.0066], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGvgCQn2fTTY"
      },
      "source": [
        "Ahora podemos usar el modelo que hemos definido. Veamos si funciona correctamente: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ6FGOvYfTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb01372-d82d-492a-daf1-2a3abd64e3f4"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    break\n",
        "\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "outputs.shape :  torch.Size([128, 10])\n",
            "Sample outputs :\n",
            " tensor([[ 0.0242, -0.1473, -0.1323, -0.4522, -0.1788,  0.1086,  0.1633, -0.2029,\n",
            "         -0.0179, -0.3169],\n",
            "        [-0.1232, -0.1008, -0.1989, -0.0561, -0.3552,  0.1934,  0.3474,  0.0849,\n",
            "         -0.1237, -0.1934]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v407S1jfTTZ"
      },
      "source": [
        "Para cada una de las 128 imagenes, ahora tenemos 10 salidas, una por cada clase. Como se ha indicado anteriormente, tomaremos esos valores de salida como posibilidades. Para ello, los valores de salida deben estar entre el rango 0 y 1, lo cual no es el caso. \n",
        "\n",
        "Para realizar la transformación de los valores obtenidos a probabilidades, emplearemos la **función sofmax**, que tiene la siguiente fórmula: \n",
        "\n",
        "![softmax](https://i.imgur.com/EAh9jLN.png)\n",
        "\n",
        "Primero reemplazamos cada elemento `yi` de una fila de salida en `e^yi`, haciendo todos los elementos positivos. \n",
        "\n",
        "![](https://www.montereyinstitute.org/courses/DevelopmentalMath/COURSE_TEXT2_RESOURCE/U18_L1_T1_text_final_6_files/image001.png)\n",
        "\n",
        "A continuación dividimos cada uno entre la suma de todos, para asegurar que no superamos el valor 1. El vector vestor resultante ya puede ser interpretado como probabilidades.\n",
        "\n",
        "A pesar de que implementar directamente la función sofmax no es dificil, Pytorch ya cuenta con una implementación propia que además funciona muy bien con tensores multidimensionales (p.e. una lista de filas de salida en nuestro caso)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc7d6WD0fTTZ"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbYF4auWfTTZ"
      },
      "source": [
        "La función softmax está incluidoa en el paquete `torch.nn.functional` y requiere que le especifiquemos la dimensión a lo largo de la cual debe ser aplicada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xvjcBVTo14n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5b7528-c1cb-411c-fc8a-b3d2e7ae2c56"
      },
      "source": [
        "outputs[:2]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0242, -0.1473, -0.1323, -0.4522, -0.1788,  0.1086,  0.1633, -0.2029,\n",
              "         -0.0179, -0.3169],\n",
              "        [-0.1232, -0.1008, -0.1989, -0.0561, -0.3552,  0.1934,  0.3474,  0.0849,\n",
              "         -0.1237, -0.1934]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3neZ1YGfTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0096d50-14b5-4167-e4d3-a51607b95521"
      },
      "source": [
        "# Aplicamos softmax para cada fila de salida\n",
        "probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "# Miramos las probabilidades de unas muestras\n",
        "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
        "\n",
        "# Comprobamos la suma de probabilidades de una fila:\n",
        "print(\"Sum: \", torch.sum(probs[0]).item())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample probabilities:\n",
            " tensor([[0.1131, 0.0953, 0.0967, 0.0703, 0.0924, 0.1231, 0.1300, 0.0902, 0.1085,\n",
            "         0.0804],\n",
            "        [0.0913, 0.0934, 0.0847, 0.0977, 0.0724, 0.1254, 0.1462, 0.1125, 0.0913,\n",
            "         0.0851]])\n",
            "Sum:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2cgqWGVfTTZ"
      },
      "source": [
        "Finalmente elegiremos la etiqueta de salida simplemente seleccionando el índice de la que mayores probabilidades ha obtenido en cada fila (vector) de salida. Para ello podemos usar `torch.max`, que retorna para cada fila, el mayor elemento y su correspondiente índice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o9a4hAufTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc96300f-bfbd-46aa-8da7-e2e36bb227de"
      },
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)\n",
        "print(max_probs)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6, 6, 6, 6, 6, 2, 5, 5, 3, 6, 5, 5, 5, 5, 6, 0, 8, 8, 6, 5, 6, 5, 6, 3,\n",
            "        5, 6, 8, 5, 6, 0, 5, 0, 8, 6, 5, 6, 5, 8, 0, 6, 8, 6, 6, 5, 1, 3, 6, 5,\n",
            "        5, 5, 5, 1, 8, 7, 5, 8, 5, 0, 5, 5, 6, 6, 5, 5, 6, 5, 5, 5, 8, 8, 5, 8,\n",
            "        5, 5, 5, 6, 6, 7, 5, 0, 5, 5, 6, 6, 8, 6, 8, 6, 2, 5, 6, 6, 2, 5, 1, 5,\n",
            "        5, 5, 5, 8, 6, 5, 8, 0, 6, 5, 3, 5, 5, 5, 6, 8, 5, 8, 1, 8, 5, 6, 5, 0,\n",
            "        5, 8, 0, 5, 2, 5, 5, 6])\n",
            "tensor([0.1300, 0.1462, 0.1459, 0.1371, 0.1306, 0.1245, 0.1399, 0.1746, 0.1240,\n",
            "        0.1176, 0.1266, 0.1370, 0.1385, 0.1566, 0.1157, 0.1293, 0.1329, 0.1198,\n",
            "        0.1229, 0.1368, 0.1376, 0.1501, 0.1348, 0.1420, 0.1225, 0.1325, 0.1369,\n",
            "        0.1336, 0.1361, 0.1324, 0.1413, 0.1191, 0.1636, 0.1204, 0.1317, 0.1282,\n",
            "        0.1257, 0.1256, 0.1212, 0.1549, 0.1249, 0.1339, 0.1204, 0.1532, 0.1149,\n",
            "        0.1263, 0.1236, 0.1281, 0.1289, 0.1308, 0.1435, 0.1179, 0.1375, 0.1138,\n",
            "        0.1583, 0.1386, 0.1225, 0.1300, 0.1518, 0.1324, 0.1398, 0.1258, 0.1285,\n",
            "        0.1211, 0.1400, 0.1294, 0.1529, 0.1225, 0.1388, 0.1485, 0.1278, 0.1284,\n",
            "        0.1349, 0.1511, 0.1247, 0.1216, 0.1401, 0.1223, 0.1249, 0.1275, 0.1311,\n",
            "        0.1472, 0.1250, 0.1276, 0.1188, 0.1516, 0.1178, 0.1426, 0.1504, 0.1399,\n",
            "        0.1202, 0.1423, 0.1224, 0.1383, 0.1187, 0.1403, 0.1226, 0.1755, 0.1383,\n",
            "        0.1190, 0.1434, 0.1370, 0.1195, 0.1113, 0.1297, 0.1567, 0.1305, 0.1554,\n",
            "        0.1624, 0.1192, 0.1370, 0.1594, 0.1145, 0.1203, 0.1128, 0.1540, 0.1162,\n",
            "        0.1462, 0.1354, 0.1219, 0.1404, 0.1200, 0.1228, 0.1326, 0.1289, 0.1244,\n",
            "        0.1257, 0.1244], grad_fn=<MaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGvKME9WfTTZ"
      },
      "source": [
        "Los números mostrados arriba son las etiquetas predichas para el primer lote de entrenamiento. Vamos a compararlas con las reales. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsT85mhffTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee82efc-ff30-4692-c886-7191044ad6c5"
      },
      "source": [
        "labels"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 8, 7, 6, 4, 0, 2, 3, 6, 0, 1, 7, 5, 3, 2, 5, 5, 2, 0, 3, 4, 7, 7, 0,\n",
              "        1, 8, 5, 5, 9, 2, 8, 3, 4, 4, 5, 5, 1, 8, 2, 4, 4, 3, 2, 7, 1, 5, 5, 6,\n",
              "        0, 9, 9, 1, 3, 5, 7, 6, 3, 5, 7, 5, 6, 8, 5, 5, 6, 7, 9, 1, 5, 6, 8, 3,\n",
              "        3, 7, 8, 5, 0, 2, 9, 2, 3, 7, 8, 2, 2, 5, 1, 7, 9, 9, 2, 9, 6, 2, 5, 8,\n",
              "        0, 7, 3, 1, 2, 1, 3, 5, 7, 7, 9, 7, 8, 1, 8, 4, 0, 1, 1, 8, 2, 5, 7, 1,\n",
              "        5, 6, 2, 0, 6, 2, 1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AFZgI8SfTTZ"
      },
      "source": [
        "La mayoría de las etiquetas de predicción son diferentes de las reales. Esto es porque hemos empezado con unos valores de pesos y biases elegidos al azar. Evidentemente, necesitamos entrenar el modelo para ajustar los pesos y realizar mejores predicciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FegT9vhlfTTZ"
      },
      "source": [
        "## Métrica de evaluación y función de error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSiZ3KtqfTTZ"
      },
      "source": [
        "De igual forma que en la regresión lineal necesitamos una forma para evaluar como de bien está haciendolo nuestro modelo. Una forma básica y muy natural consiste en obtener el porcentaje de etiquetas que son correctamente predichas, i.e. la **precision** (**accuracy**)  de las predicciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG0IBPVMp3UE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15adfd6d-b61a-4a00-9906-93e17ca36f12"
      },
      "source": [
        "outputs[:2]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0242, -0.1473, -0.1323, -0.4522, -0.1788,  0.1086,  0.1633, -0.2029,\n",
              "         -0.0179, -0.3169],\n",
              "        [-0.1232, -0.1008, -0.1989, -0.0561, -0.3552,  0.1934,  0.3474,  0.0849,\n",
              "         -0.1237, -0.1934]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHgMnCL4qGDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a793cc-34a6-4aab-ed10-aa66e62db9b1"
      },
      "source": [
        "torch.sum(preds == labels)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11hDOTR1fTTZ"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)) #suma de todos los preds que coincidan con las etiquetas, en porcentaje"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E258dVOUfTTZ"
      },
      "source": [
        "El  operador `==` realiza una comparación elemento por elemento de los dos tensores que tienen la misma forma y retorna un tensor de resultados de la misma forma, conteniendo `True` para elementos iguales y `False` para los diferentes. Pasando el resultado a `torch.sum` nos retorna el número total de etiquetas que se han predicho correctamente. Finalmente dividimos entre el numero total de etiquetas evaluadas para obtener la precisión. \n",
        "\n",
        "Nótese que en este caso no es necesario aplicar la función softmax al vector de resultados. Esto es así porque `e^x` es una función incremental, i.e., si `y1 > y2`, entonces `e^y1 > e^y2`. Lo mismo se cumple despues de promediar los valores para obtener el softmax. Por tanto podemos saltarnos esa transformación tomando directamente el valor máximo del vector de salida.\n",
        "\n",
        "Calculamos la precision del modelo para el primer lote de datos (sin entrenar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwVAxpZXfTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76779796-a8b6-45d1-baec-45be3536be6a"
      },
      "source": [
        "accuracy(outputs, labels)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1250)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEyaxGIBrNFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ccd244-68b4-470e-ae02-190e3841968a"
      },
      "source": [
        "probs"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1131, 0.0953, 0.0967,  ..., 0.0902, 0.1085, 0.0804],\n",
              "        [0.0913, 0.0934, 0.0847,  ..., 0.1125, 0.0913, 0.0851],\n",
              "        [0.0985, 0.0663, 0.1026,  ..., 0.0931, 0.0983, 0.0701],\n",
              "        ...,\n",
              "        [0.1213, 0.0968, 0.0967,  ..., 0.1047, 0.1034, 0.0737],\n",
              "        [0.0933, 0.1011, 0.0739,  ..., 0.0981, 0.1083, 0.1028],\n",
              "        [0.0845, 0.0987, 0.0794,  ..., 0.0870, 0.1219, 0.0896]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJG_F1aqgmQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307cba41-310f-4a29-d0f5-8320caf3dc1a"
      },
      "source": [
        "outputs"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0242, -0.1473, -0.1323,  ..., -0.2029, -0.0179, -0.3169],\n",
              "        [-0.1232, -0.1008, -0.1989,  ...,  0.0849, -0.1237, -0.1934],\n",
              "        [-0.0351, -0.4306,  0.0057,  ..., -0.0910, -0.0364, -0.3752],\n",
              "        ...,\n",
              "        [ 0.1527, -0.0730, -0.0741,  ...,  0.0050, -0.0075, -0.3458],\n",
              "        [-0.1384, -0.0577, -0.3715,  ..., -0.0881,  0.0105, -0.0411],\n",
              "        [-0.1558, -0.0010, -0.2187,  ..., -0.1267,  0.2106, -0.0978]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te7ORtqVfTTZ"
      },
      "source": [
        "La precisión es una buena forma de evaluar el modelo para nosotros (como humanos). Sin embargo, no la podemos emplear para optimizar el modelo empleando la técnica de descenso de gradiente por las siguientes razones: \n",
        "\n",
        "1. No es una función diferenciable. Tanto `torch.max` como `==` son operaciones no-continuas y no-diferenciables. Por tanto no podemos calcular gradientes con repecto a los pesos y los biases, que nos puedan servir para actualizar los valores en la dirección correcta.\n",
        "\n",
        "2. No tiene en cuenta las probabilidades obtenidas por el modelo, lo que hace que no podamos proveer de suficiente realimentación al modelo para mejoras incrementales. \n",
        "\n",
        "Por esas razones, la precision es utilizada habitualemente como **métrica de evaluacion** pero no como función de pérdida (tambien llamada función de coste). La función de pérdida comúnmente empleada en problemas de clasificación es la función de **entropía cruzada**  (_cross-entropy_), que tiene la siguiente fórmula: \n",
        "\n",
        "![cross-entropy](https://i.imgur.com/VDRDl1D.png)\n",
        "\n",
        "Aunque pueda parecer un poco complicada, en realidad es bastante simple:\n",
        "\n",
        "* Para cada vector de salida, tomamos el valor de probabilidad obtenido para la etiqueta correcta. E.g., si las probailidades obtenidas para una imagen son `[0.1, 0.3, 0.2, ...]` y la etiqueta correcta es `1`, tomaremos el valor `0.3` e inoramos el resto\n",
        "\n",
        "* A continuación obtenemos el [logaritmo](https://en.wikipedia.org/wiki/Logarithm) del valor de probailidad obtenida.  Si la probabilidad es alta, i.e. cercana a 1, entonces el valor obtenido será cercano a 0. Si la probabilidad es baja (cercana a 0) entonces el valor obtenido será un valor negativo alto. Dicho valor lo multiplicaremos por -1 para convertirlo en positivo. Esto resulta en que cuando la predicción es baja obtenemos un valor alto que será tomado como la pérdida o error. Cuando la predicción es alta, el error estará cercano a 0. \n",
        "\n",
        "![](https://www.intmath.com/blog/wp-content/images/2019/05/log10.png)\n",
        "\n",
        "* Finalmente para aprovechar la información de todas las muestras empleadas, tomaremos el promedio de entropía cruzada de todas las filas de salida y tenemos una medida de la pérdida para todo un lote de datos. \n",
        "\n",
        "A diferencia de la precisión. la entropía cruzada es continua y diferenciable. También provee una información de feedback muy útil para esas mejoras incrementales necesarias (mayores probabilidades se traducen en menor pérdida o error). Esas características hacen que esta función sea una buena elección como función de coste para entrenar el modelo. \n",
        "\n",
        "Como era de esperar Pytorch tiene una implementación propia eficiente y preparada para tensores como parte del paquete `torch.nn.functional`. Más aún, tiene implementado internamente la función softmax, por lo que podemos pasarle directamente la salida del modelo sin tener que convertirlo en probavilidades. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkkyxKR1r8nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ed4c55-e62f-4e5c-8d99-6153f4717b9e"
      },
      "source": [
        "outputs"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0242, -0.1473, -0.1323,  ..., -0.2029, -0.0179, -0.3169],\n",
              "        [-0.1232, -0.1008, -0.1989,  ...,  0.0849, -0.1237, -0.1934],\n",
              "        [-0.0351, -0.4306,  0.0057,  ..., -0.0910, -0.0364, -0.3752],\n",
              "        ...,\n",
              "        [ 0.1527, -0.0730, -0.0741,  ...,  0.0050, -0.0075, -0.3458],\n",
              "        [-0.1384, -0.0577, -0.3715,  ..., -0.0881,  0.0105, -0.0411],\n",
              "        [-0.1558, -0.0010, -0.2187,  ..., -0.1267,  0.2106, -0.0978]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqWUQpANfTTZ"
      },
      "source": [
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuESG0hbfTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ac1405-05f9-4dba-a84e-85ab0d43d4aa"
      },
      "source": [
        "# Pérdida para el lote actual de datos\n",
        "loss = loss_fn(outputs, labels)\n",
        "print(loss)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2819, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFOtpxUqfTTZ"
      },
      "source": [
        "Ahora sabemos que la entropía cruzada es el negativo del logaritmo de la probabilidad predicha para la etiqueta correcta, promediado sobre todas las muestras. Por tanto, una forma en la que podemos interpretar el número resultante obtenido e.g. `2.23` es que `e^-2.23` que es en torno a `0.1` como la probabilidad promedio de acierto de etiqueta correcta. En definitica *cuanto menor pérdida o error, mejor es el modelo*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTgl76MgfTTZ"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Ahora que ya hemos definido la carga de datos, función de coste y optimizador, estamos listos para entrenar el modelo. El proceso es idéntico al de la regresión lineal, con el añadido de que ahora tenemos una fase de validación para evaluar el modeloo en cada ciclo/epoch. \n",
        "\n",
        "Veamos como queda esto en pseudocódigo:\n",
        "\n",
        "```\n",
        "for epoch in range(num_epochs):\n",
        "    # Fase de entrenamiento\n",
        "    for batch in train_loader:\n",
        "        # Genera predicciones\n",
        "        # Calcula el error/pérdida\n",
        "        # Calcula los gradientes c.r.a. los pesos y biases\n",
        "        # Actualiza los pesos\n",
        "        # Reseteamos los gradientes a cero \n",
        "    # Fase de validación\n",
        "    for batch in val_loader:\n",
        "        # Genera predictiones\n",
        "        # Calcula perdida\n",
        "        # Calcula metricas (accuracy etc.)\n",
        "    # Calcula promediado de loss & metricas\n",
        "    \n",
        "    # Log epoch, loss & metrics para inspecion\n",
        "```\n",
        "\n",
        "Algunas partes del bucle de entrenamiento son específicas del problema que estamos enfrentando (e.g. loss function, metricas etc.) mientras otras son genéricas y pueden ser aplicadas a cualquier problema de aprendizaje máquina. \n",
        "\n",
        "Lo que vamos a hacer es incluir las partes independientes del problema en una función que llamaremos `fit` y que emplearemos para entrenar el modelo. Las partes más específicas las implementaremos añaiendo métodos a la clase base  `nn.Module` que definiremos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cv7a7ukfTTZ"
      },
      "source": [
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    history = [] # Aquí guardaremos resultados por epochs\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        # Entrenamiento \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step() #actualiza los pesos\n",
        "            optimizer.zero_grad() #resetea\n",
        "        \n",
        "        # Validación\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAuhlro7fTTZ"
      },
      "source": [
        "En esta función `fit`  vamos a grabar la pérdida de validación y la métrica para cada epoch. Nos devuelve un histórico del entrenamiento realizado, lo cual será útil para visualizar y deputar. \n",
        "\n",
        "Configuraciones como el tamaño de lote, tasa de aprendizaje, etc. (llamados hiperparámetros) son críticos para la obtención de buenos modelos y tiempos de entrenamiento razonables. Son un área activa de investigación por sí solos. Resulta interesante (y normalmente necesario) ir probando con ellospara ver como afecta al proceso y resultados de entrenmiento.\n",
        "\n",
        "Definimos ahora una función `evaluate` que emplearemos en la fase de validación de la function `fit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N064K5H9ujtQ"
      },
      "source": [
        "l1 = [1, 2, 3, 4, 5]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W8a_hmFujqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c729491-ba7f-4407-b43f-50c897ed9654"
      },
      "source": [
        "l2 = [x*2 for x in l1]\n",
        "l2"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgWjXs5mfTTZ"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdcKIXvMfTTZ"
      },
      "source": [
        "Finalmente vamos a redefinir la clase `MnistModel` para incluir una serie de métodos adicionales `training_step`, `validation_step`, `validation_epoch_end`, y `epoch_end` que aprovecharemos en las funciones `fit` y `evaluate`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbJN7ICGfTTZ"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784)\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Genera predicciones (lineal)\n",
        "        loss = F.cross_entropy(out, labels) # Calcula loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Genera predicciones\n",
        "        loss = F.cross_entropy(out, labels)   # Calcula loss\n",
        "        acc = accuracy(out, labels)           # Calcula accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combina losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combina accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
        "    \n",
        "model = MnistModel()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-omw1RBfTTZ"
      },
      "source": [
        "Antes de entrenar nuestro nuevo modelo, vamos a ver que resultados obtiene con los pesos y biases inicializados aleatoriamente en la fase inicial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUBy9qTfTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "974d7aa2-59e3-402d-eb89-7d8fe5daa2d5"
      },
      "source": [
        "result0 = evaluate(model, val_loader) #no hemos entrenado nada aun\n",
        "result0"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.1015625, 'val_loss': 2.3345391750335693}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KapQW0CDfTTZ"
      },
      "source": [
        "La precisión inicial es en torno al 10%, que es lo que uno puede esperar de un modelo aleatorio teniendo en cuenta que tenemos 10 valores posibles de salida ( es decir, una posibilidad entre 10 de acertar al escoger aleatoriamente).\n",
        "\n",
        "Vamos a entrenar ahora el modelo. Configuramos 5 epochs inicialmente y observamos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQRahsa6fTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2200c179-158e-4181-9027-b629b7f44856"
      },
      "source": [
        "history1 = fit(5, 0.01, model, train_loader, val_loader) #5 ciclos con learning rate de 0.01"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], val_loss: 0.8916, val_acc: 0.8323\n",
            "Epoch [1], val_loss: 0.6715, val_acc: 0.8547\n",
            "Epoch [2], val_loss: 0.5816, val_acc: 0.8621\n",
            "Epoch [3], val_loss: 0.5311, val_acc: 0.8657\n",
            "Epoch [4], val_loss: 0.4982, val_acc: 0.8725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mcIbHuKfTTa"
      },
      "source": [
        "Ahora tenemos un gran resultado! Simplemente hemos entrenado 5 ciclos y ya estamos en torno al 80% en el validation set. Veamos si epodemos mejorar más entrenando algunos ciclos más. Prueba a cambiar el número de ciclos o learning rate en las celdas siguientes y observa los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJvMd9CufTTa"
      },
      "source": [
        "history2 = fit(10, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hON9V8GBfTTa"
      },
      "source": [
        "history3 = fit(5, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5ALmwZjfTTa"
      },
      "source": [
        "history4 = fit(5, 0.0001, model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCm1903IfTTa"
      },
      "source": [
        "Aunque la precisión va mejorando cuando continuamos el entrenamiento, las mejoras son cada vez más pequeñas. \n",
        "\n",
        "Veamos esto en un gráfico:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swNRQSa0fTTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c2b75fd9-ed86-4d48-ee2b-3c9d96d98364"
      },
      "source": [
        "history = [result0] + history1 #+ history2 + history3 + history4\n",
        "accuracies = [result['val_acc'] for result in history]\n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs');"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-a7zRGxfTTa"
      },
      "source": [
        "A la vista de los resultados obtenidos y del gráfico, se aprecia fácilmente que probablemente nuestro modelo no pueda ir mucho más allá de un 90% de precisión, incluso después de entrenar bastantes ciclos. Una posible explicación para esto es que tengamos una tasa de aprendizaje (_learning rate_) alta y sea muy dificil acceder a esas pequeñas mejoras debido al tamaño del  \"paso\" (recordemos el gráfico de la curva del error y el gradiente). Los valores obtenidos pueden estar dando vueltas alrededor del punto óptimo con el mínimo error. Se debe probar diferentes tasas de aprendizaje durante más ciclos de entrenamiento para observar si eso mejora. \n",
        "\n",
        "Sin embargo, otra razón, probablemente más importante es que **el modelo no es lo bastante potente**. Si recordamos la hipótesis inicial planteada al plantear el modelo, hemos asumido que la salida (en este caso las probabilidades de cada clase de salida) es una **función lineal** de las entradas (intensidades de los píxels), ponderadas por unos pesos y on un factor de ajuste al que llamamos bias. Pero eso es una presunción muy débil, pues no tiene porqué existir una relacion lineal entre la intensidad de los píxels en una imagen y el número que representan. Es por ello que aunque funciona razonablemente bien con un pequeño conjunto como MNIST (se ha obtenido en torno al 85%), necesitaremos modelos más sofísticados que sean capaces de captar también relaciones no lineales entre las variables de entrada y sus etiquetas, de forma que puedan ser aplicados en tareas más complejas como clasificación avanzada, reconocimiento de objetos, personas, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXgOorOIfTTa"
      },
      "source": [
        "## Probando con imágenes individuales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qUQKIJefTTa"
      },
      "source": [
        "Una vez que hemos explorado los resultados y la precisión del modelo entrenado, también es necesario ver que resultados produce sobre imágenes nuevas. \n",
        "Vamos a probar el modelo con algunas del conjunto de test de 10000 imágenes que como sabemos, no ha intervenido en ningun momento en el entrenamiento. Empezaremos por generar el test dataset empleando el metodo transform de `ToTensor`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqcHUQK3fTTa"
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root='data/', \n",
        "                     train=False,\n",
        "                     transform=transforms.ToTensor())"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw9J_viefTTa"
      },
      "source": [
        "Revisamos algun eejemplo del conjunto test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzEdFHq6fTTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "9359f1cf-c023-4a3a-d98b-2258bc5c2485"
      },
      "source": [
        "img, label = test_dataset[100]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Shape:', img.shape)\n",
        "print('Label:', label)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([1, 28, 28])\n",
            "Label: 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgwbsDUjfTTa"
      },
      "source": [
        "Podemos ahora definir una función auxiliar `predict_image`, que devolverá la etiqueta obtenida para una sola imagen, proporcionada en un tensor individual. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FfeYQaOfTTa"
      },
      "source": [
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds = torch.max(yb, dim=1)\n",
        "    return preds[0].item()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ppgosBYfTTa"
      },
      "source": [
        "`img.unsqueeze` lo que hace es añadir una dimensión adicional al principio del tensor de la imagen 1x28x28, convirtiendolo en un tensor de 1x1x28x28, que el modelo interpretarla como un lote conteniendo una sola imagen\n",
        "\n",
        "Probamos con varias imágenes: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPNcp52ifTTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a43c7f12-2953-449d-e9a6-ca92fc29422c"
      },
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 7 , Predicted: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM3ElEQVR4nO3dXahc9bnH8d/vpCmI6UXiS9ik0bTBC8tBEo1BSCxbQktOvIjFIM1FyYHi7kWUFkuo2It4WaQv1JvALkrTkmMJpGoQscmJxVDU4o5Es2NIjCGaxLxYIjQRJMY+vdjLso0za8ZZa2ZN8nw/sJmZ9cya9bDMz7VmvczfESEAV77/aroBAINB2IEkCDuQBGEHkiDsQBJfGeTCbHPoH+iziHCr6ZW27LZX2j5o+7Dth6t8FoD+cq/n2W3PkHRI0nckHZf0mqS1EfFWyTxs2YE+68eWfamkwxFxJCIuSPqTpNUVPg9AH1UJ+zxJx6a9Pl5M+xzbY7YnbE9UWBaAivp+gC4ixiWNS+zGA02qsmU/IWn+tNdfL6YBGEJVwv6apJtsf8P2VyV9X9L2etoCULeed+Mj4qLtByT9RdIMSU9GxP7aOgNQq55PvfW0ML6zA33Xl4tqAFw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9Dw+uyTZPirpnKRPJV2MiCV1NAWgfpXCXrgrIv5Rw+cA6CN244EkqoY9JO2wvcf2WKs32B6zPWF7ouKyAFTgiOh9ZnteRJywfb2knZIejIjdJe/vfWEAuhIRbjW90pY9Ik4Uj2ckPS1paZXPA9A/PYfd9tW2v/bZc0nflTRZV2MA6lXlaPxcSU/b/uxz/i8iXqilKwC1q/Sd/UsvjO/sQN/15Ts7gMsHYQeSIOxAEoQdSIKwA0nUcSNMCmvWrGlbu//++0vnff/990vrH3/8cWl9y5YtpfVTp061rR0+fLh0XuTBlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuCuty4dOXKkbW3BggWDa6SFc+fOta3t379/gJ0Ml+PHj7etPfbYY6XzTkxcvr+ixl1vQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE97N3qeye9VtuuaV03gMHDpTWb7755tL6rbfeWlofHR1tW7vjjjtK5z127Fhpff78+aX1Ki5evFha/+CDD0rrIyMjPS/7vffeK61fzufZ22HLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcD/7FWD27Nlta4sWLSqdd8+ePaX122+/vaeeutHp9/IPHTpUWu90/cKcOXPa1tavX18676ZNm0rrw6zn+9ltP2n7jO3JadPm2N5p++3isf2/NgBDoZvd+N9LWnnJtIcl7YqImyTtKl4DGGIdwx4RuyWdvWTyakmbi+ebJd1Tc18AatbrtfFzI+Jk8fyUpLnt3mh7TNJYj8sBUJPKN8JERJQdeIuIcUnjEgfogCb1eurttO0RSSoez9TXEoB+6DXs2yWtK56vk/RsPe0A6JeO59ltPyVpVNK1kk5L2ijpGUlbJd0g6V1J90XEpQfxWn0Wu/Ho2r333lta37p1a2l9cnKybe2uu+4qnffs2Y7/nIdWu/PsHb+zR8TaNqUVlToCMFBcLgskQdiBJAg7kARhB5Ig7EAS3OKKxlx//fWl9X379lWaf82aNW1r27ZtK533csaQzUByhB1IgrADSRB2IAnCDiRB2IEkCDuQBEM2ozGdfs75uuuuK61/+OGHpfWDBw9+6Z6uZGzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ7mdHXy1btqxt7cUXXyydd+bMmaX10dHR0vru3btL61cq7mcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4nx19tWrVqra1TufRd+3aVVp/5ZVXeuopq45bdttP2j5je3LatEdtn7C9t/hr/18UwFDoZjf+95JWtpj+m4hYVPw9X29bAOrWMewRsVvS2QH0AqCPqhyge8D2m8Vu/ux2b7I9ZnvC9kSFZQGoqNewb5K0UNIiSScl/ardGyNiPCKWRMSSHpcFoAY9hT0iTkfEpxHxL0m/k7S03rYA1K2nsNsemfbye5Im270XwHDoeJ7d9lOSRiVda/u4pI2SRm0vkhSSjkr6UR97xBC76qqrSusrV7Y6kTPlwoULpfNu3LixtP7JJ5+U1vF5HcMeEWtbTH6iD70A6CMulwWSIOxAEoQdSIKwA0kQdiAJbnFFJRs2bCitL168uG3thRdeKJ335Zdf7qkntMaWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYMhmlLr77rtL688880xp/aOPPmpbK7v9VZJeffXV0jpaY8hmIDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC+9mTu+aaa0rrjz/+eGl9xowZpfXnn28/5ifn0QeLLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH97Fe4TufBO53rvu2220rr77zzTmm97J71TvOiNz3fz257vu2/2n7L9n7bPy6mz7G90/bbxePsupsGUJ9uduMvSvppRHxL0h2S1tv+lqSHJe2KiJsk7SpeAxhSHcMeEScj4vXi+TlJByTNk7Ra0ubibZsl3dOvJgFU96Wujbe9QNJiSX+XNDciThalU5LmtplnTNJY7y0CqEPXR+Ntz5K0TdJPIuKf02sxdZSv5cG3iBiPiCURsaRSpwAq6SrstmdqKuhbIuLPxeTTtkeK+oikM/1pEUAdOu7G27akJyQdiIhfTyttl7RO0i+Kx2f70iEqWbhwYWm906m1Th566KHSOqfXhkc339mXSfqBpH229xbTHtFUyLfa/qGkdyXd158WAdShY9gj4m+SWp6kl7Si3nYA9AuXywJJEHYgCcIOJEHYgSQIO5AEPyV9Bbjxxhvb1nbs2FHpszds2FBaf+655yp9PgaHLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMF59ivA2Fj7X/264YYbKn32Sy+9VFof5E+Roxq27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOfZLwPLly8vrT/44IMD6gSXM7bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEN+Ozz5f0B0lzJYWk8Yj4re1HJd0v6YPirY9ExPP9ajSzO++8s7Q+a9asnj+70/jp58+f7/mzMVy6uajmoqSfRsTrtr8maY/tnUXtNxHxy/61B6Au3YzPflLSyeL5OdsHJM3rd2MA6vWlvrPbXiBpsaS/F5MesP2m7Sdtz24zz5jtCdsTlToFUEnXYbc9S9I2ST+JiH9K2iRpoaRFmtry/6rVfBExHhFLImJJDf0C6FFXYbc9U1NB3xIRf5akiDgdEZ9GxL8k/U7S0v61CaCqjmG3bUlPSDoQEb+eNn1k2tu+J2my/vYA1KWbo/HLJP1A0j7be4tpj0haa3uRpk7HHZX0o750iEreeOON0vqKFStK62fPnq2zHTSom6Pxf5PkFiXOqQOXEa6gA5Ig7EAShB1IgrADSRB2IAnCDiThQQ65a5vxfYE+i4hWp8rZsgNZEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoMesvkfkt6d9vraYtowGtbehrUvid56VWdvN7YrDPSimi8s3J4Y1t+mG9behrUvid56Naje2I0HkiDsQBJNh3284eWXGdbehrUvid56NZDeGv3ODmBwmt6yAxgQwg4k0UjYba+0fdD2YdsPN9FDO7aP2t5ne2/T49MVY+idsT05bdoc2zttv108thxjr6HeHrV9olh3e22vaqi3+bb/avst2/tt/7iY3ui6K+lrIOtt4N/Zbc+QdEjSdyQdl/SapLUR8dZAG2nD9lFJSyKi8QswbH9b0nlJf4iI/y6mPSbpbET8ovgf5eyI+NmQ9PaopPNND+NdjFY0Mn2YcUn3SPpfNbjuSvq6TwNYb01s2ZdKOhwRRyLigqQ/SVrdQB9DLyJ2S7p0SJbVkjYXzzdr6h/LwLXpbShExMmIeL14fk7SZ8OMN7ruSvoaiCbCPk/SsWmvj2u4xnsPSTts77E91nQzLcyNiJPF81OS5jbZTAsdh/EepEuGGR+addfL8OdVcYDui5ZHxK2S/kfS+mJ3dSjF1HewYTp32tUw3oPSYpjx/2hy3fU6/HlVTYT9hKT5015/vZg2FCLiRPF4RtLTGr6hqE9/NoJu8Xim4X7+Y5iG8W41zLiGYN01Ofx5E2F/TdJNtr9h+6uSvi9pewN9fIHtq4sDJ7J9taTvaviGot4uaV3xfJ2kZxvs5XOGZRjvdsOMq+F11/jw5xEx8D9JqzR1RP4dST9vooc2fX1T0hvF3/6me5P0lKZ26z7R1LGNH0q6RtIuSW9L+n9Jc4aotz9K2ifpTU0Fa6Sh3pZrahf9TUl7i79VTa+7kr4Gst64XBZIggN0QBKEHUiCsANJEHYgCcIOJEHYgSQIO5DEvwEvYRv57rmVLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrj-w1COfTTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "86d4d757-a32c-4d06-e56c-147a4a544ae2"
      },
      "source": [
        "img, label = test_dataset[10]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 0 , Predicted: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPrF7gB4fTTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6df30b31-4953-425b-8033-addb584ffedb"
      },
      "source": [
        "img, label = test_dataset[193]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 9 , Predicted: 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eetYJdoFfTTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "43ec67e5-e718-40f5-9924-a81b37c90fd8"
      },
      "source": [
        "img, label = test_dataset[1839]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 2 , Predicted: 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ6Wsk4TfTTb"
      },
      "source": [
        "Identificando donde lo hace peor nuestro modelo nos puede orientar a mejorar el modelo bien recogiendo más datos de entrenamiento, ajaustando los hiperparámetros, incrementando/ o reduciendo la complejidad del modelo, etc.\n",
        "\n",
        "Como paso final, vamos a obtener los valores de loss y accuracy del modelo sobre todo el conjunto de test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bdjaeG-fTTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87825f43-31e8-4ade-ad2f-4a53f7717187"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.8829101324081421, 'val_loss': 0.46748772263526917}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfe2IzF3fTTb"
      },
      "source": [
        "Los resultados obtenidos con el conjunto de test nos pueden orientar sobre las  formas de afrontar una mejora. Si los resultados son muy distintos, es posible que el conjunto de test no esté correctamente muestreado y por tanto no tenga similar distribución a los empleados en el entrenamiento. También se pueden detectar situaciones de sobreajuste en el entrenamiento. En general, valores similares en entrenamiento y test indican que al menos metodológicamente el modelo está bien planteado, si bien puede necesitar mejoras en parámetros, arquitectura, etc. para aumentar sus resultados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# El resultado del entrenamiento lo tenemos en la lista history: \n",
        "history[-1]"
      ],
      "metadata": {
        "id": "IfSgaB1eGHcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6108cd-3e8b-405d-87e6-f402d80710f1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.8725277185440063, 'val_loss': 0.4981725811958313}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqC3HEZJfTTb"
      },
      "source": [
        "## Salvar y cargar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyE7vx_zfTTb"
      },
      "source": [
        "Una vez entrenado el modelo (lo cual puede haber requerido mucho tiempo) y obtenido unos resultados aceptables, es importante poder guardar los pesos y biases obtenidos, de forma que podamos recuperarlos y reutilizarlos cuando queramos, evitando así tener que reentrenar desde cero. \n",
        "\n",
        "Una forma de grabar el modelo será la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xESC-106fTTb"
      },
      "source": [
        "torch.save(model.state_dict(), 'mnist-logistic.pth')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAHSyFLPfTTb"
      },
      "source": [
        "El método `.state_dict` retorna un `OrderedDict` conteniendo las matrices con los pesos y biases mapeados del modelo que hemos especificado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elbp48SCfTTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9c8b34-eb75-4fc6-97ec-764fcb6e0b62"
      },
      "source": [
        "model.state_dict() #diccionario con el tipo de modelo y ajuste que ha hecho y los pesos"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0200,  0.0053, -0.0034,  ...,  0.0291,  0.0277,  0.0094],\n",
              "                      [-0.0296, -0.0157, -0.0199,  ..., -0.0237,  0.0004,  0.0140],\n",
              "                      [ 0.0184, -0.0097,  0.0193,  ..., -0.0353, -0.0047, -0.0301],\n",
              "                      ...,\n",
              "                      [ 0.0257, -0.0051, -0.0316,  ...,  0.0343, -0.0250,  0.0146],\n",
              "                      [-0.0356,  0.0044, -0.0350,  ..., -0.0069, -0.0198,  0.0097],\n",
              "                      [ 0.0302,  0.0231,  0.0048,  ..., -0.0233,  0.0055, -0.0207]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0430,  0.1307, -0.0299, -0.0653,  0.0752,  0.1417, -0.0127,  0.0995,\n",
              "                      -0.1493, -0.0343]))])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PKRLwDPV3nK"
      },
      "source": [
        "**NOTA IMPORTANTE**: si ejecutas el método `torch.sav`en Colab, el fichero generado se graba temporalmente en el directorio de datos del cuaderno. Será necesario descargarlo a un equipo local o trasladarlo a otra ubicación permanente o se perderá cuando finalicemos la sesión. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc9kRTDpfTTb"
      },
      "source": [
        "Para cargar los parámetros del modelo, podemos instanciar un nuevo objeto de la clase inicial `MnistModel`, y a continuación emplear el método `.load_state_dict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO666r7_1rbW"
      },
      "source": [
        "model2 = MnistModel()"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUvYwe4Q1six",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbd057d-bc68-4dd7-e31d-f6c687e8a513"
      },
      "source": [
        "# inicalmente los parámetros son los que se han adjudicado aleatoriamente al instanciar el objeto\n",
        "model2.state_dict()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[ 0.0317, -0.0219, -0.0324,  ..., -0.0224,  0.0179,  0.0065],\n",
              "                      [-0.0035,  0.0227,  0.0006,  ...,  0.0280,  0.0010, -0.0339],\n",
              "                      [-0.0273, -0.0218, -0.0139,  ...,  0.0085,  0.0352,  0.0342],\n",
              "                      ...,\n",
              "                      [ 0.0107, -0.0040,  0.0356,  ...,  0.0260, -0.0240, -0.0129],\n",
              "                      [ 0.0294, -0.0284, -0.0158,  ..., -0.0258,  0.0292, -0.0246],\n",
              "                      [-0.0277,  0.0279, -0.0146,  ..., -0.0027, -0.0055,  0.0162]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0303,  0.0346,  0.0332,  0.0072,  0.0246, -0.0040,  0.0307, -0.0192,\n",
              "                      -0.0173, -0.0105]))])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHac9s6e1vyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b648ef38-e936-41c1-900b-bf623e07d992"
      },
      "source": [
        "evaluate(model2, test_loader)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.09492187201976776, 'val_loss': 2.324634313583374}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvR1g8ggfTTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36390933-311b-4d86-90a6-581ad52e2148"
      },
      "source": [
        "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
        "model2.state_dict()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0200,  0.0053, -0.0034,  ...,  0.0291,  0.0277,  0.0094],\n",
              "                      [-0.0296, -0.0157, -0.0199,  ..., -0.0237,  0.0004,  0.0140],\n",
              "                      [ 0.0184, -0.0097,  0.0193,  ..., -0.0353, -0.0047, -0.0301],\n",
              "                      ...,\n",
              "                      [ 0.0257, -0.0051, -0.0316,  ...,  0.0343, -0.0250,  0.0146],\n",
              "                      [-0.0356,  0.0044, -0.0350,  ..., -0.0069, -0.0198,  0.0097],\n",
              "                      [ 0.0302,  0.0231,  0.0048,  ..., -0.0233,  0.0055, -0.0207]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0430,  0.1307, -0.0299, -0.0653,  0.0752,  0.1417, -0.0127,  0.0995,\n",
              "                      -0.1493, -0.0343]))])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhjBm4BMfTTb"
      },
      "source": [
        "Podemos comprobar que los parámetros que tenemos son los que habiamos grabado, y comprobamos que obtenemos en los mismos resultados de loss y accuracy en el conjunto de test, igual que antes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UynZ4aSLfTTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f29daa-6768-4b89-b9d2-51e9cda113ad"
      },
      "source": [
        "#test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "result = evaluate(model2, test_loader)\n",
        "result"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.8829101324081421, 'val_loss': 0.46748772263526917}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnz8WyThaPOP"
      },
      "source": [
        "Observamos que los valores obtenidos son exactamente los mismos que obtuvimos al realizar la evaluación con el modelo `model` varias celdas más arriba, lo cual indica que los pesos se han cargado correctamente y que nuestro `modelo2` lo está haciendo tan bien como el modelo original de donde proceden los valores de los parámetros. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mas datasets para clasificación: CIFAR10\n",
        "El conjunto de datasets que nos proporciona la librería `torchvision`es amplio e interesante. Puedes ver un tutorial con el dataset CIFAR10 [aquí](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). \n",
        "\n",
        "Este conjunto tiene imágenes de 10 tipos de vehículos y animales (‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’) y su complejidad permite explorar más a fondo tanto técnicas de regresión logística como redes neuronales más complejas."
      ],
      "metadata": {
        "id": "wbiKjzmo361P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2QOf-HEltUP"
      },
      "source": [
        "# Fin del Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZDr1zYmBkRV"
      },
      "source": [
        "Referencias y modelos empleados para el Notebook: \n",
        "\n",
        "*   Documentación de [Pytorch](https://pytorch.org/docs/stable/index.html) \n",
        "*   [PyTorch Tutorial for Deep Learning Researchers](https://github.com/yunjey/pytorch-tutorial) by Yunjey Choi\n",
        "*   [FastAI](https://www.fast.ai/) development notebooks by Jeremy Howard.\n",
        "*   Documentación y cursos en [Pierian Data](https://www.pieriandata.com/)\n",
        "*   Tutoriales y notebooks del curso \"Deep Learning with Pytorch: Zero to GANs\" de [Aakash N S](https://jovian.ai/aakashns)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}